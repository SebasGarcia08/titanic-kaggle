{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>embarked</th>\n",
       "      <th>age_group</th>\n",
       "      <th>familysz</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>19.091437</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   age     fare  sibsp  parch  pclass  sex  embarked  \\\n",
       "PassengerId                                                            \n",
       "1            22.000000   7.2500    1.0    0.0       2    1         2   \n",
       "2            38.000000  71.2833    1.0    0.0       0    0         0   \n",
       "3            26.000000   7.9250    0.0    0.0       2    0         2   \n",
       "4            35.000000  53.1000    1.0    0.0       0    0         2   \n",
       "5            35.000000   8.0500    0.0    0.0       2    1         2   \n",
       "...                ...      ...    ...    ...     ...  ...       ...   \n",
       "886          39.000000  29.1250    0.0    5.0       2    0         1   \n",
       "888          19.000000  30.0000    0.0    0.0       0    0         2   \n",
       "889          19.091437  23.4500    1.0    2.0       2    0         2   \n",
       "890          26.000000  30.0000    0.0    0.0       0    1         0   \n",
       "891          32.000000   7.7500    0.0    0.0       2    1         1   \n",
       "\n",
       "             age_group  familysz  survived  \n",
       "PassengerId                                 \n",
       "1                    4         1         0  \n",
       "2                    0         1         1  \n",
       "3                    4         1         1  \n",
       "4                    0         1         1  \n",
       "5                    0         1         0  \n",
       "...                ...       ...       ...  \n",
       "886                  0         0         0  \n",
       "888                  3         1         1  \n",
       "889                  3         0         0  \n",
       "890                  4         1         1  \n",
       "891                  0         1         0  \n",
       "\n",
       "[756 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data/'\n",
    "df = pd.read_csv('../data/processed/train.csv', index_col='PassengerId')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 3, 5, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "cat_dims = [df[col].nunique() for col in df.columns[4:]]\n",
    "print(cat_dims)\n",
    "cat_idxs = list(range(4, len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.583333\n",
       "1    0.416667\n",
       "Name: survived, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['survived'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complejidad 3 (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Desarrolle un modelo que permita obtener los valores de la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2785828985\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 5.77944 | train_unsup_loss: 161.8956| val_unsup_loss: 111.24139|  0:00:00s\n",
      "epoch 1  | loss: 4.6224  | train_unsup_loss: 32.93577| val_unsup_loss: 26.372  |  0:00:00s\n",
      "epoch 2  | loss: 3.81354 | train_unsup_loss: 22.22789| val_unsup_loss: 14.74843|  0:00:00s\n",
      "epoch 3  | loss: 2.87812 | train_unsup_loss: 15.39487| val_unsup_loss: 16.60109|  0:00:00s\n",
      "epoch 4  | loss: 2.76621 | train_unsup_loss: 12.34333| val_unsup_loss: 9.80925 |  0:00:00s\n",
      "epoch 5  | loss: 2.53548 | train_unsup_loss: 11.56439| val_unsup_loss: 7.98727 |  0:00:00s\n",
      "epoch 6  | loss: 2.33322 | train_unsup_loss: 8.9364  | val_unsup_loss: 5.58788 |  0:00:01s\n",
      "epoch 7  | loss: 2.07655 | train_unsup_loss: 4.7163  | val_unsup_loss: 4.00575 |  0:00:01s\n",
      "epoch 8  | loss: 2.04682 | train_unsup_loss: 4.25163 | val_unsup_loss: 3.60495 |  0:00:01s\n",
      "epoch 9  | loss: 2.01675 | train_unsup_loss: 3.58374 | val_unsup_loss: 3.00165 |  0:00:01s\n",
      "epoch 10 | loss: 1.78976 | train_unsup_loss: 4.80753 | val_unsup_loss: 3.5509  |  0:00:01s\n",
      "epoch 11 | loss: 1.76558 | train_unsup_loss: 5.42995 | val_unsup_loss: 3.88213 |  0:00:01s\n",
      "epoch 12 | loss: 1.91154 | train_unsup_loss: 4.35034 | val_unsup_loss: 3.32103 |  0:00:01s\n",
      "epoch 13 | loss: 1.63879 | train_unsup_loss: 3.60172 | val_unsup_loss: 2.63796 |  0:00:02s\n",
      "epoch 14 | loss: 1.64843 | train_unsup_loss: 2.6562  | val_unsup_loss: 2.36502 |  0:00:02s\n",
      "epoch 15 | loss: 1.68804 | train_unsup_loss: 2.45099 | val_unsup_loss: 2.2345  |  0:00:02s\n",
      "epoch 16 | loss: 1.71881 | train_unsup_loss: 2.29084 | val_unsup_loss: 2.11813 |  0:00:02s\n",
      "epoch 17 | loss: 1.52116 | train_unsup_loss: 2.18849 | val_unsup_loss: 2.01768 |  0:00:02s\n",
      "epoch 18 | loss: 1.62735 | train_unsup_loss: 2.14758 | val_unsup_loss: 1.97568 |  0:00:02s\n",
      "epoch 19 | loss: 1.55907 | train_unsup_loss: 2.17345 | val_unsup_loss: 1.99726 |  0:00:02s\n",
      "epoch 20 | loss: 1.52147 | train_unsup_loss: 2.10973 | val_unsup_loss: 1.97352 |  0:00:02s\n",
      "epoch 21 | loss: 1.4308  | train_unsup_loss: 2.07176 | val_unsup_loss: 1.95083 |  0:00:03s\n",
      "epoch 22 | loss: 1.53887 | train_unsup_loss: 2.04719 | val_unsup_loss: 1.90814 |  0:00:03s\n",
      "epoch 23 | loss: 1.51919 | train_unsup_loss: 1.91753 | val_unsup_loss: 1.82834 |  0:00:03s\n",
      "epoch 24 | loss: 1.49838 | train_unsup_loss: 1.84409 | val_unsup_loss: 1.7601  |  0:00:03s\n",
      "epoch 25 | loss: 1.46585 | train_unsup_loss: 1.68745 | val_unsup_loss: 1.63936 |  0:00:03s\n",
      "epoch 26 | loss: 1.43044 | train_unsup_loss: 1.61712 | val_unsup_loss: 1.58629 |  0:00:03s\n",
      "epoch 27 | loss: 1.50814 | train_unsup_loss: 1.55114 | val_unsup_loss: 1.54464 |  0:00:03s\n",
      "epoch 28 | loss: 1.509   | train_unsup_loss: 1.52575 | val_unsup_loss: 1.52664 |  0:00:04s\n",
      "epoch 29 | loss: 1.32982 | train_unsup_loss: 1.51859 | val_unsup_loss: 1.51906 |  0:00:04s\n",
      "epoch 30 | loss: 1.38728 | train_unsup_loss: 1.53672 | val_unsup_loss: 1.52825 |  0:00:04s\n",
      "epoch 31 | loss: 1.49185 | train_unsup_loss: 1.54875 | val_unsup_loss: 1.53305 |  0:00:04s\n",
      "epoch 32 | loss: 1.38965 | train_unsup_loss: 1.54905 | val_unsup_loss: 1.52635 |  0:00:04s\n",
      "epoch 33 | loss: 1.35529 | train_unsup_loss: 1.52875 | val_unsup_loss: 1.51341 |  0:00:04s\n",
      "epoch 34 | loss: 1.39637 | train_unsup_loss: 1.51014 | val_unsup_loss: 1.49875 |  0:00:04s\n",
      "epoch 35 | loss: 1.36697 | train_unsup_loss: 1.4974  | val_unsup_loss: 1.48442 |  0:00:04s\n",
      "epoch 36 | loss: 1.40056 | train_unsup_loss: 1.48631 | val_unsup_loss: 1.47031 |  0:00:05s\n",
      "epoch 37 | loss: 1.35504 | train_unsup_loss: 1.48206 | val_unsup_loss: 1.45878 |  0:00:05s\n",
      "epoch 38 | loss: 1.33512 | train_unsup_loss: 1.46708 | val_unsup_loss: 1.45077 |  0:00:05s\n",
      "epoch 39 | loss: 1.38756 | train_unsup_loss: 1.44579 | val_unsup_loss: 1.42995 |  0:00:05s\n",
      "epoch 40 | loss: 1.34501 | train_unsup_loss: 1.43208 | val_unsup_loss: 1.42239 |  0:00:05s\n",
      "epoch 41 | loss: 1.43224 | train_unsup_loss: 1.4244  | val_unsup_loss: 1.42136 |  0:00:05s\n",
      "epoch 42 | loss: 1.35977 | train_unsup_loss: 1.42067 | val_unsup_loss: 1.42694 |  0:00:05s\n",
      "epoch 43 | loss: 1.32043 | train_unsup_loss: 1.42208 | val_unsup_loss: 1.43236 |  0:00:05s\n",
      "epoch 44 | loss: 1.40236 | train_unsup_loss: 1.43956 | val_unsup_loss: 1.44534 |  0:00:05s\n",
      "epoch 45 | loss: 1.31547 | train_unsup_loss: 1.44546 | val_unsup_loss: 1.44813 |  0:00:06s\n",
      "epoch 46 | loss: 1.33479 | train_unsup_loss: 1.43358 | val_unsup_loss: 1.43906 |  0:00:06s\n",
      "epoch 47 | loss: 1.41461 | train_unsup_loss: 1.42723 | val_unsup_loss: 1.43131 |  0:00:06s\n",
      "epoch 48 | loss: 1.2629  | train_unsup_loss: 1.41557 | val_unsup_loss: 1.42086 |  0:00:06s\n",
      "epoch 49 | loss: 1.30017 | train_unsup_loss: 1.40562 | val_unsup_loss: 1.41336 |  0:00:06s\n",
      "epoch 50 | loss: 1.29627 | train_unsup_loss: 1.408   | val_unsup_loss: 1.40951 |  0:00:06s\n",
      "epoch 51 | loss: 1.21611 | train_unsup_loss: 1.40903 | val_unsup_loss: 1.40997 |  0:00:06s\n",
      "epoch 52 | loss: 1.27393 | train_unsup_loss: 1.41321 | val_unsup_loss: 1.41317 |  0:00:06s\n",
      "epoch 53 | loss: 1.27415 | train_unsup_loss: 1.41127 | val_unsup_loss: 1.40821 |  0:00:07s\n",
      "epoch 54 | loss: 1.11929 | train_unsup_loss: 1.38746 | val_unsup_loss: 1.38723 |  0:00:07s\n",
      "epoch 55 | loss: 1.31993 | train_unsup_loss: 1.36083 | val_unsup_loss: 1.37618 |  0:00:07s\n",
      "epoch 56 | loss: 1.1869  | train_unsup_loss: 1.33604 | val_unsup_loss: 1.36089 |  0:00:07s\n",
      "epoch 57 | loss: 1.26402 | train_unsup_loss: 1.31034 | val_unsup_loss: 1.33757 |  0:00:07s\n",
      "epoch 58 | loss: 1.24461 | train_unsup_loss: 1.29061 | val_unsup_loss: 1.31637 |  0:00:07s\n",
      "epoch 59 | loss: 1.17723 | train_unsup_loss: 1.28279 | val_unsup_loss: 1.30735 |  0:00:07s\n",
      "epoch 60 | loss: 1.25716 | train_unsup_loss: 1.26749 | val_unsup_loss: 1.29569 |  0:00:07s\n",
      "epoch 61 | loss: 1.17112 | train_unsup_loss: 1.25429 | val_unsup_loss: 1.28408 |  0:00:08s\n",
      "epoch 62 | loss: 1.14696 | train_unsup_loss: 1.23961 | val_unsup_loss: 1.26073 |  0:00:08s\n",
      "epoch 63 | loss: 1.12576 | train_unsup_loss: 1.2414  | val_unsup_loss: 1.26084 |  0:00:08s\n",
      "epoch 64 | loss: 1.10684 | train_unsup_loss: 1.23958 | val_unsup_loss: 1.2513  |  0:00:08s\n",
      "epoch 65 | loss: 1.08685 | train_unsup_loss: 1.23557 | val_unsup_loss: 1.25121 |  0:00:08s\n",
      "epoch 66 | loss: 1.17846 | train_unsup_loss: 1.21869 | val_unsup_loss: 1.23571 |  0:00:08s\n",
      "epoch 67 | loss: 1.06299 | train_unsup_loss: 1.20103 | val_unsup_loss: 1.21793 |  0:00:08s\n",
      "epoch 68 | loss: 1.10696 | train_unsup_loss: 1.17915 | val_unsup_loss: 1.19998 |  0:00:08s\n",
      "epoch 69 | loss: 1.0166  | train_unsup_loss: 1.15659 | val_unsup_loss: 1.17715 |  0:00:09s\n",
      "epoch 70 | loss: 1.10943 | train_unsup_loss: 1.13761 | val_unsup_loss: 1.15132 |  0:00:09s\n",
      "epoch 71 | loss: 1.05468 | train_unsup_loss: 1.12392 | val_unsup_loss: 1.1414  |  0:00:09s\n",
      "epoch 72 | loss: 1.07198 | train_unsup_loss: 1.11502 | val_unsup_loss: 1.13097 |  0:00:09s\n",
      "epoch 73 | loss: 1.05048 | train_unsup_loss: 1.10464 | val_unsup_loss: 1.1262  |  0:00:09s\n",
      "epoch 74 | loss: 0.99098 | train_unsup_loss: 1.09503 | val_unsup_loss: 1.11898 |  0:00:09s\n",
      "epoch 75 | loss: 1.09159 | train_unsup_loss: 1.08872 | val_unsup_loss: 1.11172 |  0:00:09s\n",
      "epoch 76 | loss: 1.06598 | train_unsup_loss: 1.08263 | val_unsup_loss: 1.09963 |  0:00:09s\n",
      "epoch 77 | loss: 1.05733 | train_unsup_loss: 1.08271 | val_unsup_loss: 1.09115 |  0:00:09s\n",
      "epoch 78 | loss: 0.95729 | train_unsup_loss: 1.08124 | val_unsup_loss: 1.08944 |  0:00:10s\n",
      "epoch 79 | loss: 1.05338 | train_unsup_loss: 1.05622 | val_unsup_loss: 1.07475 |  0:00:10s\n",
      "epoch 80 | loss: 0.86012 | train_unsup_loss: 1.04455 | val_unsup_loss: 1.06457 |  0:00:10s\n",
      "epoch 81 | loss: 0.99748 | train_unsup_loss: 1.03899 | val_unsup_loss: 1.06097 |  0:00:10s\n",
      "epoch 82 | loss: 0.92551 | train_unsup_loss: 1.04953 | val_unsup_loss: 1.07107 |  0:00:10s\n",
      "epoch 83 | loss: 0.9947  | train_unsup_loss: 1.04861 | val_unsup_loss: 1.06752 |  0:00:10s\n",
      "epoch 84 | loss: 0.92592 | train_unsup_loss: 1.02949 | val_unsup_loss: 1.04715 |  0:00:10s\n",
      "epoch 85 | loss: 0.97184 | train_unsup_loss: 1.01876 | val_unsup_loss: 1.04046 |  0:00:10s\n",
      "epoch 86 | loss: 0.93711 | train_unsup_loss: 1.01199 | val_unsup_loss: 1.03378 |  0:00:11s\n",
      "epoch 87 | loss: 0.97316 | train_unsup_loss: 1.01097 | val_unsup_loss: 1.02496 |  0:00:11s\n",
      "epoch 88 | loss: 0.8986  | train_unsup_loss: 1.01454 | val_unsup_loss: 1.02213 |  0:00:11s\n",
      "epoch 89 | loss: 0.90871 | train_unsup_loss: 1.00621 | val_unsup_loss: 1.01998 |  0:00:11s\n",
      "epoch 90 | loss: 0.91568 | train_unsup_loss: 0.99961 | val_unsup_loss: 1.02442 |  0:00:11s\n",
      "epoch 91 | loss: 0.90167 | train_unsup_loss: 1.00506 | val_unsup_loss: 1.03655 |  0:00:11s\n",
      "epoch 92 | loss: 0.9111  | train_unsup_loss: 1.01052 | val_unsup_loss: 1.03849 |  0:00:11s\n",
      "epoch 93 | loss: 0.8609  | train_unsup_loss: 1.02182 | val_unsup_loss: 1.03627 |  0:00:11s\n",
      "epoch 94 | loss: 0.84563 | train_unsup_loss: 1.01821 | val_unsup_loss: 1.02974 |  0:00:12s\n",
      "epoch 95 | loss: 0.79525 | train_unsup_loss: 1.00179 | val_unsup_loss: 1.01889 |  0:00:12s\n",
      "epoch 96 | loss: 0.87667 | train_unsup_loss: 0.99028 | val_unsup_loss: 1.01245 |  0:00:12s\n",
      "epoch 97 | loss: 0.85876 | train_unsup_loss: 0.98972 | val_unsup_loss: 1.00636 |  0:00:12s\n",
      "epoch 98 | loss: 0.83909 | train_unsup_loss: 0.99238 | val_unsup_loss: 1.00506 |  0:00:12s\n",
      "epoch 99 | loss: 0.89931 | train_unsup_loss: 0.99554 | val_unsup_loss: 1.00266 |  0:00:12s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_unsup_loss = 1.00266\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 0.75366 | train_auc: 0.6336  | val_auc: 0.58391 |  0:00:00s\n",
      "epoch 1  | loss: 0.61885 | train_auc: 0.65701 | val_auc: 0.60835 |  0:00:00s\n",
      "epoch 2  | loss: 0.58905 | train_auc: 0.67462 | val_auc: 0.62921 |  0:00:00s\n",
      "epoch 3  | loss: 0.58684 | train_auc: 0.69867 | val_auc: 0.64402 |  0:00:00s\n",
      "epoch 4  | loss: 0.57539 | train_auc: 0.72392 | val_auc: 0.65739 |  0:00:00s\n",
      "epoch 5  | loss: 0.55217 | train_auc: 0.74292 | val_auc: 0.66078 |  0:00:00s\n",
      "epoch 6  | loss: 0.53278 | train_auc: 0.74802 | val_auc: 0.67184 |  0:00:00s\n",
      "epoch 7  | loss: 0.52306 | train_auc: 0.75707 | val_auc: 0.67933 |  0:00:00s\n",
      "epoch 8  | loss: 0.51412 | train_auc: 0.757   | val_auc: 0.69395 |  0:00:01s\n",
      "epoch 9  | loss: 0.51046 | train_auc: 0.75313 | val_auc: 0.68718 |  0:00:01s\n",
      "epoch 10 | loss: 0.51901 | train_auc: 0.75324 | val_auc: 0.68254 |  0:00:01s\n",
      "epoch 11 | loss: 0.5035  | train_auc: 0.74537 | val_auc: 0.67202 |  0:00:01s\n",
      "epoch 12 | loss: 0.49201 | train_auc: 0.74147 | val_auc: 0.66363 |  0:00:01s\n",
      "epoch 13 | loss: 0.48032 | train_auc: 0.74458 | val_auc: 0.66363 |  0:00:01s\n",
      "epoch 14 | loss: 0.48646 | train_auc: 0.73742 | val_auc: 0.67576 |  0:00:01s\n",
      "epoch 15 | loss: 0.47938 | train_auc: 0.73865 | val_auc: 0.66881 |  0:00:01s\n",
      "epoch 16 | loss: 0.45992 | train_auc: 0.74338 | val_auc: 0.67826 |  0:00:01s\n",
      "epoch 17 | loss: 0.46324 | train_auc: 0.76034 | val_auc: 0.68575 |  0:00:02s\n",
      "epoch 18 | loss: 0.45582 | train_auc: 0.77825 | val_auc: 0.69823 |  0:00:02s\n",
      "epoch 19 | loss: 0.46147 | train_auc: 0.79462 | val_auc: 0.70804 |  0:00:02s\n",
      "epoch 20 | loss: 0.45455 | train_auc: 0.8022  | val_auc: 0.7224  |  0:00:02s\n",
      "epoch 21 | loss: 0.45235 | train_auc: 0.8042  | val_auc: 0.72971 |  0:00:02s\n",
      "epoch 22 | loss: 0.4287  | train_auc: 0.80994 | val_auc: 0.73952 |  0:00:02s\n",
      "epoch 23 | loss: 0.44127 | train_auc: 0.8129  | val_auc: 0.75548 |  0:00:02s\n",
      "epoch 24 | loss: 0.44133 | train_auc: 0.81562 | val_auc: 0.75887 |  0:00:02s\n",
      "epoch 25 | loss: 0.44208 | train_auc: 0.81948 | val_auc: 0.75156 |  0:00:02s\n",
      "epoch 26 | loss: 0.41855 | train_auc: 0.82061 | val_auc: 0.74755 |  0:00:02s\n",
      "epoch 27 | loss: 0.43128 | train_auc: 0.82046 | val_auc: 0.74523 |  0:00:03s\n",
      "epoch 28 | loss: 0.42559 | train_auc: 0.81361 | val_auc: 0.74229 |  0:00:03s\n",
      "epoch 29 | loss: 0.42515 | train_auc: 0.8088  | val_auc: 0.73248 |  0:00:03s\n",
      "epoch 30 | loss: 0.44182 | train_auc: 0.80726 | val_auc: 0.73515 |  0:00:03s\n",
      "epoch 31 | loss: 0.42237 | train_auc: 0.811   | val_auc: 0.74764 |  0:00:03s\n",
      "epoch 32 | loss: 0.42765 | train_auc: 0.82244 | val_auc: 0.75459 |  0:00:03s\n",
      "epoch 33 | loss: 0.41722 | train_auc: 0.82803 | val_auc: 0.76743 |  0:00:03s\n",
      "epoch 34 | loss: 0.41671 | train_auc: 0.82885 | val_auc: 0.77278 |  0:00:03s\n",
      "epoch 35 | loss: 0.41388 | train_auc: 0.82068 | val_auc: 0.77029 |  0:00:03s\n",
      "epoch 36 | loss: 0.41191 | train_auc: 0.79973 | val_auc: 0.75049 |  0:00:03s\n",
      "epoch 37 | loss: 0.41313 | train_auc: 0.78757 | val_auc: 0.74264 |  0:00:03s\n",
      "epoch 38 | loss: 0.40915 | train_auc: 0.78453 | val_auc: 0.73836 |  0:00:04s\n",
      "epoch 39 | loss: 0.39963 | train_auc: 0.78966 | val_auc: 0.73141 |  0:00:04s\n",
      "epoch 40 | loss: 0.404   | train_auc: 0.82654 | val_auc: 0.74639 |  0:00:04s\n",
      "epoch 41 | loss: 0.39562 | train_auc: 0.83855 | val_auc: 0.74086 |  0:00:04s\n",
      "epoch 42 | loss: 0.39885 | train_auc: 0.83362 | val_auc: 0.72267 |  0:00:04s\n",
      "epoch 43 | loss: 0.39376 | train_auc: 0.8343  | val_auc: 0.72053 |  0:00:04s\n",
      "epoch 44 | loss: 0.38759 | train_auc: 0.83755 | val_auc: 0.72374 |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_auc = 0.77278\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 5.80947 | train_unsup_loss: 141.52559| val_unsup_loss: 123.67011|  0:00:00s\n",
      "epoch 1  | loss: 4.72701 | train_unsup_loss: 15.73965| val_unsup_loss: 15.35812|  0:00:00s\n",
      "epoch 2  | loss: 3.71114 | train_unsup_loss: 13.6965 | val_unsup_loss: 13.06335|  0:00:00s\n",
      "epoch 3  | loss: 3.12696 | train_unsup_loss: 12.98835| val_unsup_loss: 10.87712|  0:00:00s\n",
      "epoch 4  | loss: 2.91545 | train_unsup_loss: 5.40416 | val_unsup_loss: 5.52228 |  0:00:00s\n",
      "epoch 5  | loss: 2.86479 | train_unsup_loss: 3.88464 | val_unsup_loss: 5.23839 |  0:00:00s\n",
      "epoch 6  | loss: 2.60542 | train_unsup_loss: 3.12751 | val_unsup_loss: 3.59676 |  0:00:00s\n",
      "epoch 7  | loss: 2.23411 | train_unsup_loss: 3.18814 | val_unsup_loss: 3.64294 |  0:00:01s\n",
      "epoch 8  | loss: 2.11417 | train_unsup_loss: 3.51768 | val_unsup_loss: 4.10122 |  0:00:01s\n",
      "epoch 9  | loss: 2.06865 | train_unsup_loss: 3.65607 | val_unsup_loss: 4.08845 |  0:00:01s\n",
      "epoch 10 | loss: 1.90032 | train_unsup_loss: 5.36451 | val_unsup_loss: 7.17143 |  0:00:01s\n",
      "epoch 11 | loss: 1.92961 | train_unsup_loss: 6.09569 | val_unsup_loss: 7.97419 |  0:00:01s\n",
      "epoch 12 | loss: 1.9584  | train_unsup_loss: 7.83956 | val_unsup_loss: 10.32149|  0:00:01s\n",
      "epoch 13 | loss: 1.77495 | train_unsup_loss: 6.99777 | val_unsup_loss: 9.08101 |  0:00:01s\n",
      "epoch 14 | loss: 1.72532 | train_unsup_loss: 4.92636 | val_unsup_loss: 6.12187 |  0:00:01s\n",
      "epoch 15 | loss: 1.74514 | train_unsup_loss: 3.48181 | val_unsup_loss: 4.22899 |  0:00:02s\n",
      "epoch 16 | loss: 1.74789 | train_unsup_loss: 3.05659 | val_unsup_loss: 3.80003 |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_unsup_loss = 3.59676\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 0.70728 | train_auc: 0.47622 | val_auc: 0.47289 |  0:00:00s\n",
      "epoch 1  | loss: 0.66152 | train_auc: 0.51978 | val_auc: 0.55    |  0:00:00s\n",
      "epoch 2  | loss: 0.64564 | train_auc: 0.51234 | val_auc: 0.57976 |  0:00:00s\n",
      "epoch 3  | loss: 0.62667 | train_auc: 0.52723 | val_auc: 0.54588 |  0:00:00s\n",
      "epoch 4  | loss: 0.6089  | train_auc: 0.52655 | val_auc: 0.52042 |  0:00:00s\n",
      "epoch 5  | loss: 0.57766 | train_auc: 0.53396 | val_auc: 0.49341 |  0:00:00s\n",
      "epoch 6  | loss: 0.55871 | train_auc: 0.52011 | val_auc: 0.5272  |  0:00:01s\n",
      "epoch 7  | loss: 0.55342 | train_auc: 0.54109 | val_auc: 0.58498 |  0:00:01s\n",
      "epoch 8  | loss: 0.52868 | train_auc: 0.57397 | val_auc: 0.63626 |  0:00:01s\n",
      "epoch 9  | loss: 0.52142 | train_auc: 0.60469 | val_auc: 0.65485 |  0:00:01s\n",
      "epoch 10 | loss: 0.51231 | train_auc: 0.64063 | val_auc: 0.71731 |  0:00:01s\n",
      "epoch 11 | loss: 0.50413 | train_auc: 0.68556 | val_auc: 0.73452 |  0:00:01s\n",
      "epoch 12 | loss: 0.4941  | train_auc: 0.70782 | val_auc: 0.76108 |  0:00:01s\n",
      "epoch 13 | loss: 0.47854 | train_auc: 0.7103  | val_auc: 0.78498 |  0:00:01s\n",
      "epoch 14 | loss: 0.47195 | train_auc: 0.71208 | val_auc: 0.79066 |  0:00:01s\n",
      "epoch 15 | loss: 0.46147 | train_auc: 0.7111  | val_auc: 0.79267 |  0:00:02s\n",
      "epoch 16 | loss: 0.47078 | train_auc: 0.71777 | val_auc: 0.80201 |  0:00:02s\n",
      "epoch 17 | loss: 0.46837 | train_auc: 0.71905 | val_auc: 0.79084 |  0:00:02s\n",
      "epoch 18 | loss: 0.45758 | train_auc: 0.7237  | val_auc: 0.78956 |  0:00:02s\n",
      "epoch 19 | loss: 0.45176 | train_auc: 0.73087 | val_auc: 0.79322 |  0:00:02s\n",
      "epoch 20 | loss: 0.44869 | train_auc: 0.73646 | val_auc: 0.78141 |  0:00:02s\n",
      "epoch 21 | loss: 0.44158 | train_auc: 0.72896 | val_auc: 0.75751 |  0:00:02s\n",
      "epoch 22 | loss: 0.43064 | train_auc: 0.72586 | val_auc: 0.74853 |  0:00:02s\n",
      "epoch 23 | loss: 0.41841 | train_auc: 0.72303 | val_auc: 0.74469 |  0:00:02s\n",
      "epoch 24 | loss: 0.44009 | train_auc: 0.71738 | val_auc: 0.73993 |  0:00:03s\n",
      "epoch 25 | loss: 0.42385 | train_auc: 0.70998 | val_auc: 0.73516 |  0:00:03s\n",
      "epoch 26 | loss: 0.42862 | train_auc: 0.69769 | val_auc: 0.73095 |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_auc = 0.80201\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 5.77381 | train_unsup_loss: 120.80756| val_unsup_loss: 69.93291|  0:00:00s\n",
      "epoch 1  | loss: 5.21567 | train_unsup_loss: 57.14083| val_unsup_loss: 59.54385|  0:00:00s\n",
      "epoch 2  | loss: 3.65367 | train_unsup_loss: 14.94911| val_unsup_loss: 16.51058|  0:00:00s\n",
      "epoch 3  | loss: 3.22516 | train_unsup_loss: 11.21984| val_unsup_loss: 10.26153|  0:00:00s\n",
      "epoch 4  | loss: 3.00992 | train_unsup_loss: 9.39238 | val_unsup_loss: 7.9513  |  0:00:00s\n",
      "epoch 5  | loss: 2.84871 | train_unsup_loss: 6.42254 | val_unsup_loss: 5.79591 |  0:00:00s\n",
      "epoch 6  | loss: 2.44761 | train_unsup_loss: 3.90447 | val_unsup_loss: 3.9592  |  0:00:00s\n",
      "epoch 7  | loss: 2.25535 | train_unsup_loss: 3.2594  | val_unsup_loss: 3.25238 |  0:00:01s\n",
      "epoch 8  | loss: 2.26342 | train_unsup_loss: 3.16697 | val_unsup_loss: 2.72764 |  0:00:01s\n",
      "epoch 9  | loss: 2.00217 | train_unsup_loss: 2.90124 | val_unsup_loss: 2.88347 |  0:00:01s\n",
      "epoch 10 | loss: 1.82336 | train_unsup_loss: 2.71955 | val_unsup_loss: 2.7591  |  0:00:01s\n",
      "epoch 11 | loss: 1.87335 | train_unsup_loss: 2.77273 | val_unsup_loss: 2.8261  |  0:00:01s\n",
      "epoch 12 | loss: 1.96817 | train_unsup_loss: 3.0148  | val_unsup_loss: 2.95636 |  0:00:01s\n",
      "epoch 13 | loss: 1.74945 | train_unsup_loss: 3.11721 | val_unsup_loss: 2.89549 |  0:00:01s\n",
      "epoch 14 | loss: 1.72728 | train_unsup_loss: 2.85387 | val_unsup_loss: 2.6438  |  0:00:01s\n",
      "epoch 15 | loss: 1.79621 | train_unsup_loss: 2.45151 | val_unsup_loss: 2.42246 |  0:00:02s\n",
      "epoch 16 | loss: 1.68605 | train_unsup_loss: 2.34455 | val_unsup_loss: 2.20285 |  0:00:02s\n",
      "epoch 17 | loss: 1.6032  | train_unsup_loss: 2.14805 | val_unsup_loss: 1.98273 |  0:00:02s\n",
      "epoch 18 | loss: 1.6255  | train_unsup_loss: 2.053   | val_unsup_loss: 1.91357 |  0:00:02s\n",
      "epoch 19 | loss: 1.54463 | train_unsup_loss: 1.99079 | val_unsup_loss: 1.83617 |  0:00:02s\n",
      "epoch 20 | loss: 1.69626 | train_unsup_loss: 1.96538 | val_unsup_loss: 1.82673 |  0:00:02s\n",
      "epoch 21 | loss: 1.57159 | train_unsup_loss: 1.96508 | val_unsup_loss: 1.80634 |  0:00:02s\n",
      "epoch 22 | loss: 1.56617 | train_unsup_loss: 1.92653 | val_unsup_loss: 1.76873 |  0:00:02s\n",
      "epoch 23 | loss: 1.4411  | train_unsup_loss: 1.91875 | val_unsup_loss: 1.78142 |  0:00:03s\n",
      "epoch 24 | loss: 1.44248 | train_unsup_loss: 1.936   | val_unsup_loss: 1.80346 |  0:00:03s\n",
      "epoch 25 | loss: 1.45236 | train_unsup_loss: 1.92345 | val_unsup_loss: 1.79349 |  0:00:03s\n",
      "epoch 26 | loss: 1.48461 | train_unsup_loss: 1.86457 | val_unsup_loss: 1.74468 |  0:00:03s\n",
      "epoch 27 | loss: 1.45967 | train_unsup_loss: 1.80338 | val_unsup_loss: 1.68704 |  0:00:03s\n",
      "epoch 28 | loss: 1.47976 | train_unsup_loss: 1.7797  | val_unsup_loss: 1.66145 |  0:00:03s\n",
      "epoch 29 | loss: 1.38692 | train_unsup_loss: 1.76455 | val_unsup_loss: 1.63857 |  0:00:03s\n",
      "epoch 30 | loss: 1.46153 | train_unsup_loss: 1.74543 | val_unsup_loss: 1.63284 |  0:00:03s\n",
      "epoch 31 | loss: 1.36131 | train_unsup_loss: 1.71688 | val_unsup_loss: 1.61071 |  0:00:04s\n",
      "epoch 32 | loss: 1.50313 | train_unsup_loss: 1.69468 | val_unsup_loss: 1.58824 |  0:00:04s\n",
      "epoch 33 | loss: 1.38612 | train_unsup_loss: 1.65289 | val_unsup_loss: 1.5496  |  0:00:04s\n",
      "epoch 34 | loss: 1.41361 | train_unsup_loss: 1.61317 | val_unsup_loss: 1.51138 |  0:00:04s\n",
      "epoch 35 | loss: 1.45346 | train_unsup_loss: 1.59316 | val_unsup_loss: 1.49857 |  0:00:04s\n",
      "epoch 36 | loss: 1.45928 | train_unsup_loss: 1.58943 | val_unsup_loss: 1.49289 |  0:00:04s\n",
      "epoch 37 | loss: 1.40104 | train_unsup_loss: 1.60419 | val_unsup_loss: 1.49904 |  0:00:04s\n",
      "epoch 38 | loss: 1.29566 | train_unsup_loss: 1.61207 | val_unsup_loss: 1.50667 |  0:00:04s\n",
      "epoch 39 | loss: 1.35758 | train_unsup_loss: 1.62019 | val_unsup_loss: 1.51021 |  0:00:05s\n",
      "epoch 40 | loss: 1.42828 | train_unsup_loss: 1.61147 | val_unsup_loss: 1.50034 |  0:00:05s\n",
      "epoch 41 | loss: 1.34376 | train_unsup_loss: 1.60133 | val_unsup_loss: 1.49409 |  0:00:05s\n",
      "epoch 42 | loss: 1.3398  | train_unsup_loss: 1.58374 | val_unsup_loss: 1.48201 |  0:00:05s\n",
      "epoch 43 | loss: 1.39033 | train_unsup_loss: 1.56528 | val_unsup_loss: 1.47037 |  0:00:05s\n",
      "epoch 44 | loss: 1.27831 | train_unsup_loss: 1.54987 | val_unsup_loss: 1.4553  |  0:00:05s\n",
      "epoch 45 | loss: 1.22931 | train_unsup_loss: 1.53864 | val_unsup_loss: 1.45247 |  0:00:05s\n",
      "epoch 46 | loss: 1.23676 | train_unsup_loss: 1.52275 | val_unsup_loss: 1.44072 |  0:00:05s\n",
      "epoch 47 | loss: 1.28813 | train_unsup_loss: 1.50924 | val_unsup_loss: 1.42705 |  0:00:06s\n",
      "epoch 48 | loss: 1.28983 | train_unsup_loss: 1.50149 | val_unsup_loss: 1.40735 |  0:00:06s\n",
      "epoch 49 | loss: 1.20422 | train_unsup_loss: 1.48943 | val_unsup_loss: 1.38948 |  0:00:06s\n",
      "epoch 50 | loss: 1.15642 | train_unsup_loss: 1.46833 | val_unsup_loss: 1.36564 |  0:00:06s\n",
      "epoch 51 | loss: 1.24897 | train_unsup_loss: 1.45214 | val_unsup_loss: 1.3481  |  0:00:06s\n",
      "epoch 52 | loss: 1.16977 | train_unsup_loss: 1.43089 | val_unsup_loss: 1.32667 |  0:00:06s\n",
      "epoch 53 | loss: 1.24614 | train_unsup_loss: 1.40337 | val_unsup_loss: 1.29888 |  0:00:06s\n",
      "epoch 54 | loss: 1.20745 | train_unsup_loss: 1.38207 | val_unsup_loss: 1.27787 |  0:00:06s\n",
      "epoch 55 | loss: 1.18508 | train_unsup_loss: 1.36367 | val_unsup_loss: 1.26034 |  0:00:07s\n",
      "epoch 56 | loss: 1.13037 | train_unsup_loss: 1.35375 | val_unsup_loss: 1.24803 |  0:00:07s\n",
      "epoch 57 | loss: 1.14297 | train_unsup_loss: 1.34632 | val_unsup_loss: 1.2391  |  0:00:07s\n",
      "epoch 58 | loss: 1.11953 | train_unsup_loss: 1.32794 | val_unsup_loss: 1.22558 |  0:00:07s\n",
      "epoch 59 | loss: 1.01248 | train_unsup_loss: 1.30934 | val_unsup_loss: 1.2195  |  0:00:07s\n",
      "epoch 60 | loss: 1.11961 | train_unsup_loss: 1.29836 | val_unsup_loss: 1.21903 |  0:00:07s\n",
      "epoch 61 | loss: 1.08596 | train_unsup_loss: 1.28556 | val_unsup_loss: 1.21871 |  0:00:07s\n",
      "epoch 62 | loss: 1.12996 | train_unsup_loss: 1.28231 | val_unsup_loss: 1.22403 |  0:00:07s\n",
      "epoch 63 | loss: 1.01278 | train_unsup_loss: 1.27739 | val_unsup_loss: 1.22568 |  0:00:08s\n",
      "epoch 64 | loss: 1.06917 | train_unsup_loss: 1.26478 | val_unsup_loss: 1.21634 |  0:00:08s\n",
      "epoch 65 | loss: 1.089   | train_unsup_loss: 1.26077 | val_unsup_loss: 1.21631 |  0:00:08s\n",
      "epoch 66 | loss: 1.03069 | train_unsup_loss: 1.25811 | val_unsup_loss: 1.21629 |  0:00:08s\n",
      "epoch 67 | loss: 0.95575 | train_unsup_loss: 1.25054 | val_unsup_loss: 1.2089  |  0:00:08s\n",
      "epoch 68 | loss: 1.01426 | train_unsup_loss: 1.23159 | val_unsup_loss: 1.20371 |  0:00:08s\n",
      "epoch 69 | loss: 0.97065 | train_unsup_loss: 1.20589 | val_unsup_loss: 1.1768  |  0:00:08s\n",
      "epoch 70 | loss: 0.95876 | train_unsup_loss: 1.19107 | val_unsup_loss: 1.16131 |  0:00:08s\n",
      "epoch 71 | loss: 0.95149 | train_unsup_loss: 1.18416 | val_unsup_loss: 1.16067 |  0:00:09s\n",
      "epoch 72 | loss: 0.98352 | train_unsup_loss: 1.17912 | val_unsup_loss: 1.16124 |  0:00:09s\n",
      "epoch 73 | loss: 0.92147 | train_unsup_loss: 1.18877 | val_unsup_loss: 1.17232 |  0:00:09s\n",
      "epoch 74 | loss: 0.96831 | train_unsup_loss: 1.17366 | val_unsup_loss: 1.15649 |  0:00:09s\n",
      "epoch 75 | loss: 0.96558 | train_unsup_loss: 1.16492 | val_unsup_loss: 1.15047 |  0:00:09s\n",
      "epoch 76 | loss: 0.90635 | train_unsup_loss: 1.15682 | val_unsup_loss: 1.14448 |  0:00:09s\n",
      "epoch 77 | loss: 0.91714 | train_unsup_loss: 1.14918 | val_unsup_loss: 1.13596 |  0:00:09s\n",
      "epoch 78 | loss: 0.95478 | train_unsup_loss: 1.12146 | val_unsup_loss: 1.10371 |  0:00:09s\n",
      "epoch 79 | loss: 0.91128 | train_unsup_loss: 1.09507 | val_unsup_loss: 1.07857 |  0:00:09s\n",
      "epoch 80 | loss: 0.93305 | train_unsup_loss: 1.0827  | val_unsup_loss: 1.06862 |  0:00:10s\n",
      "epoch 81 | loss: 0.97733 | train_unsup_loss: 1.0879  | val_unsup_loss: 1.07439 |  0:00:10s\n",
      "epoch 82 | loss: 0.99    | train_unsup_loss: 1.09947 | val_unsup_loss: 1.08461 |  0:00:10s\n",
      "epoch 83 | loss: 0.90853 | train_unsup_loss: 1.11369 | val_unsup_loss: 1.09633 |  0:00:10s\n",
      "epoch 84 | loss: 0.80282 | train_unsup_loss: 1.10753 | val_unsup_loss: 1.08715 |  0:00:10s\n",
      "epoch 85 | loss: 0.86031 | train_unsup_loss: 1.08862 | val_unsup_loss: 1.06276 |  0:00:10s\n",
      "epoch 86 | loss: 0.90783 | train_unsup_loss: 1.06496 | val_unsup_loss: 1.03976 |  0:00:10s\n",
      "epoch 87 | loss: 0.93764 | train_unsup_loss: 1.0457  | val_unsup_loss: 1.02111 |  0:00:10s\n",
      "epoch 88 | loss: 0.88539 | train_unsup_loss: 1.03787 | val_unsup_loss: 1.01102 |  0:00:11s\n",
      "epoch 89 | loss: 0.86873 | train_unsup_loss: 1.02451 | val_unsup_loss: 0.99874 |  0:00:11s\n",
      "epoch 90 | loss: 0.86567 | train_unsup_loss: 1.00411 | val_unsup_loss: 0.98243 |  0:00:11s\n",
      "epoch 91 | loss: 0.82056 | train_unsup_loss: 0.994   | val_unsup_loss: 0.96771 |  0:00:11s\n",
      "epoch 92 | loss: 0.92557 | train_unsup_loss: 0.99044 | val_unsup_loss: 0.96343 |  0:00:11s\n",
      "epoch 93 | loss: 0.88211 | train_unsup_loss: 0.99094 | val_unsup_loss: 0.96731 |  0:00:11s\n",
      "epoch 94 | loss: 0.94721 | train_unsup_loss: 0.98683 | val_unsup_loss: 0.96377 |  0:00:11s\n",
      "epoch 95 | loss: 0.93194 | train_unsup_loss: 0.97246 | val_unsup_loss: 0.94718 |  0:00:11s\n",
      "epoch 96 | loss: 0.85866 | train_unsup_loss: 0.95734 | val_unsup_loss: 0.92661 |  0:00:12s\n",
      "epoch 97 | loss: 0.86136 | train_unsup_loss: 0.94565 | val_unsup_loss: 0.91097 |  0:00:12s\n",
      "epoch 98 | loss: 0.83004 | train_unsup_loss: 0.93621 | val_unsup_loss: 0.89991 |  0:00:12s\n",
      "epoch 99 | loss: 0.88828 | train_unsup_loss: 0.93369 | val_unsup_loss: 0.90125 |  0:00:12s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_unsup_loss = 0.89991\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 0.81205 | train_auc: 0.65547 | val_auc: 0.71175 |  0:00:00s\n",
      "epoch 1  | loss: 0.66537 | train_auc: 0.66271 | val_auc: 0.69445 |  0:00:00s\n",
      "epoch 2  | loss: 0.63673 | train_auc: 0.68608 | val_auc: 0.68637 |  0:00:00s\n",
      "epoch 3  | loss: 0.61433 | train_auc: 0.70055 | val_auc: 0.70216 |  0:00:00s\n",
      "epoch 4  | loss: 0.57242 | train_auc: 0.71638 | val_auc: 0.73853 |  0:00:00s\n",
      "epoch 5  | loss: 0.54182 | train_auc: 0.73011 | val_auc: 0.74229 |  0:00:00s\n",
      "epoch 6  | loss: 0.53086 | train_auc: 0.74647 | val_auc: 0.75357 |  0:00:00s\n",
      "epoch 7  | loss: 0.52168 | train_auc: 0.76567 | val_auc: 0.77049 |  0:00:00s\n",
      "epoch 8  | loss: 0.51414 | train_auc: 0.77944 | val_auc: 0.77387 |  0:00:00s\n",
      "epoch 9  | loss: 0.50828 | train_auc: 0.78902 | val_auc: 0.77688 |  0:00:00s\n",
      "epoch 10 | loss: 0.49692 | train_auc: 0.79389 | val_auc: 0.78214 |  0:00:01s\n",
      "epoch 11 | loss: 0.48756 | train_auc: 0.80122 | val_auc: 0.78741 |  0:00:01s\n",
      "epoch 12 | loss: 0.49159 | train_auc: 0.80516 | val_auc: 0.79201 |  0:00:01s\n",
      "epoch 13 | loss: 0.49432 | train_auc: 0.80701 | val_auc: 0.78562 |  0:00:01s\n",
      "epoch 14 | loss: 0.4803  | train_auc: 0.80927 | val_auc: 0.79295 |  0:00:01s\n",
      "epoch 15 | loss: 0.47581 | train_auc: 0.81045 | val_auc: 0.79897 |  0:00:01s\n",
      "epoch 16 | loss: 0.47002 | train_auc: 0.81312 | val_auc: 0.81175 |  0:00:01s\n",
      "epoch 17 | loss: 0.47536 | train_auc: 0.81419 | val_auc: 0.8156  |  0:00:01s\n",
      "epoch 18 | loss: 0.46226 | train_auc: 0.8172  | val_auc: 0.81992 |  0:00:01s\n",
      "epoch 19 | loss: 0.46499 | train_auc: 0.81606 | val_auc: 0.82744 |  0:00:01s\n",
      "epoch 20 | loss: 0.45949 | train_auc: 0.81685 | val_auc: 0.8265  |  0:00:02s\n",
      "epoch 21 | loss: 0.45368 | train_auc: 0.81705 | val_auc: 0.81485 |  0:00:02s\n",
      "epoch 22 | loss: 0.4467  | train_auc: 0.81508 | val_auc: 0.80667 |  0:00:02s\n",
      "epoch 23 | loss: 0.45488 | train_auc: 0.81593 | val_auc: 0.80442 |  0:00:02s\n",
      "epoch 24 | loss: 0.45335 | train_auc: 0.81794 | val_auc: 0.79934 |  0:00:02s\n",
      "epoch 25 | loss: 0.46447 | train_auc: 0.81631 | val_auc: 0.80179 |  0:00:02s\n",
      "epoch 26 | loss: 0.44393 | train_auc: 0.81625 | val_auc: 0.79953 |  0:00:02s\n",
      "epoch 27 | loss: 0.4465  | train_auc: 0.81498 | val_auc: 0.7969  |  0:00:02s\n",
      "epoch 28 | loss: 0.44557 | train_auc: 0.80679 | val_auc: 0.78957 |  0:00:02s\n",
      "epoch 29 | loss: 0.448   | train_auc: 0.80321 | val_auc: 0.78976 |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_auc = 0.82744\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 5.53958 | train_unsup_loss: 126.32092| val_unsup_loss: 157.45479|  0:00:00s\n",
      "epoch 1  | loss: 3.90118 | train_unsup_loss: 78.19164| val_unsup_loss: 115.09621|  0:00:00s\n",
      "epoch 2  | loss: 3.71632 | train_unsup_loss: 102.94782| val_unsup_loss: 102.83835|  0:00:00s\n",
      "epoch 3  | loss: 3.02369 | train_unsup_loss: 20.86374| val_unsup_loss: 16.15114|  0:00:00s\n",
      "epoch 4  | loss: 2.71284 | train_unsup_loss: 14.98513| val_unsup_loss: 7.597   |  0:00:00s\n",
      "epoch 5  | loss: 2.4827  | train_unsup_loss: 5.51466 | val_unsup_loss: 7.8323  |  0:00:00s\n",
      "epoch 6  | loss: 2.25423 | train_unsup_loss: 5.52696 | val_unsup_loss: 5.80411 |  0:00:00s\n",
      "epoch 7  | loss: 2.25629 | train_unsup_loss: 4.21415 | val_unsup_loss: 5.09075 |  0:00:00s\n",
      "epoch 8  | loss: 2.08586 | train_unsup_loss: 3.73061 | val_unsup_loss: 6.69029 |  0:00:01s\n",
      "epoch 9  | loss: 1.953   | train_unsup_loss: 4.33542 | val_unsup_loss: 7.24424 |  0:00:01s\n",
      "epoch 10 | loss: 1.8695  | train_unsup_loss: 5.62132 | val_unsup_loss: 7.82698 |  0:00:01s\n",
      "epoch 11 | loss: 1.84717 | train_unsup_loss: 5.24083 | val_unsup_loss: 6.57243 |  0:00:01s\n",
      "epoch 12 | loss: 1.74387 | train_unsup_loss: 4.70401 | val_unsup_loss: 4.48443 |  0:00:01s\n",
      "epoch 13 | loss: 1.73247 | train_unsup_loss: 4.59299 | val_unsup_loss: 3.47873 |  0:00:01s\n",
      "epoch 14 | loss: 1.65175 | train_unsup_loss: 4.75535 | val_unsup_loss: 3.75222 |  0:00:01s\n",
      "epoch 15 | loss: 1.6835  | train_unsup_loss: 4.32391 | val_unsup_loss: 4.9474  |  0:00:01s\n",
      "epoch 16 | loss: 1.66923 | train_unsup_loss: 3.2714  | val_unsup_loss: 3.74787 |  0:00:02s\n",
      "epoch 17 | loss: 1.54672 | train_unsup_loss: 2.73982 | val_unsup_loss: 3.23677 |  0:00:02s\n",
      "epoch 18 | loss: 1.50325 | train_unsup_loss: 2.44662 | val_unsup_loss: 2.93507 |  0:00:02s\n",
      "epoch 19 | loss: 1.52808 | train_unsup_loss: 2.21685 | val_unsup_loss: 2.53198 |  0:00:02s\n",
      "epoch 20 | loss: 1.54899 | train_unsup_loss: 2.11896 | val_unsup_loss: 2.4005  |  0:00:02s\n",
      "epoch 21 | loss: 1.4991  | train_unsup_loss: 1.9323  | val_unsup_loss: 2.21026 |  0:00:02s\n",
      "epoch 22 | loss: 1.44445 | train_unsup_loss: 1.83042 | val_unsup_loss: 2.1814  |  0:00:02s\n",
      "epoch 23 | loss: 1.39259 | train_unsup_loss: 1.85537 | val_unsup_loss: 2.63612 |  0:00:02s\n",
      "epoch 24 | loss: 1.57267 | train_unsup_loss: 1.80487 | val_unsup_loss: 2.51439 |  0:00:03s\n",
      "epoch 25 | loss: 1.47075 | train_unsup_loss: 1.78517 | val_unsup_loss: 2.43893 |  0:00:03s\n",
      "epoch 26 | loss: 1.42176 | train_unsup_loss: 1.74215 | val_unsup_loss: 2.32605 |  0:00:03s\n",
      "epoch 27 | loss: 1.3832  | train_unsup_loss: 1.67364 | val_unsup_loss: 2.23939 |  0:00:03s\n",
      "epoch 28 | loss: 1.4725  | train_unsup_loss: 1.62534 | val_unsup_loss: 2.09451 |  0:00:03s\n",
      "epoch 29 | loss: 1.45065 | train_unsup_loss: 1.58926 | val_unsup_loss: 2.01923 |  0:00:03s\n",
      "epoch 30 | loss: 1.43087 | train_unsup_loss: 1.56123 | val_unsup_loss: 1.94678 |  0:00:03s\n",
      "epoch 31 | loss: 1.40773 | train_unsup_loss: 1.55069 | val_unsup_loss: 1.92714 |  0:00:03s\n",
      "epoch 32 | loss: 1.41498 | train_unsup_loss: 1.52384 | val_unsup_loss: 1.85817 |  0:00:04s\n",
      "epoch 33 | loss: 1.37267 | train_unsup_loss: 1.51446 | val_unsup_loss: 1.81616 |  0:00:04s\n",
      "epoch 34 | loss: 1.32423 | train_unsup_loss: 1.47873 | val_unsup_loss: 1.72556 |  0:00:04s\n",
      "epoch 35 | loss: 1.37511 | train_unsup_loss: 1.45948 | val_unsup_loss: 1.65705 |  0:00:04s\n",
      "epoch 36 | loss: 1.44131 | train_unsup_loss: 1.44716 | val_unsup_loss: 1.60873 |  0:00:04s\n",
      "epoch 37 | loss: 1.31734 | train_unsup_loss: 1.41987 | val_unsup_loss: 1.54436 |  0:00:04s\n",
      "epoch 38 | loss: 1.32664 | train_unsup_loss: 1.37923 | val_unsup_loss: 1.47859 |  0:00:04s\n",
      "epoch 39 | loss: 1.32599 | train_unsup_loss: 1.36689 | val_unsup_loss: 1.45322 |  0:00:04s\n",
      "epoch 40 | loss: 1.22681 | train_unsup_loss: 1.3604  | val_unsup_loss: 1.44331 |  0:00:05s\n",
      "epoch 41 | loss: 1.33992 | train_unsup_loss: 1.32329 | val_unsup_loss: 1.40748 |  0:00:05s\n",
      "epoch 42 | loss: 1.29725 | train_unsup_loss: 1.29471 | val_unsup_loss: 1.39324 |  0:00:05s\n",
      "epoch 43 | loss: 1.28586 | train_unsup_loss: 1.26871 | val_unsup_loss: 1.36923 |  0:00:05s\n",
      "epoch 44 | loss: 1.31803 | train_unsup_loss: 1.24876 | val_unsup_loss: 1.36072 |  0:00:05s\n",
      "epoch 45 | loss: 1.24394 | train_unsup_loss: 1.2384  | val_unsup_loss: 1.35691 |  0:00:05s\n",
      "epoch 46 | loss: 1.20811 | train_unsup_loss: 1.22421 | val_unsup_loss: 1.34355 |  0:00:05s\n",
      "epoch 47 | loss: 1.27468 | train_unsup_loss: 1.21257 | val_unsup_loss: 1.33628 |  0:00:05s\n",
      "epoch 48 | loss: 1.1511  | train_unsup_loss: 1.19368 | val_unsup_loss: 1.31444 |  0:00:06s\n",
      "epoch 49 | loss: 1.29627 | train_unsup_loss: 1.17178 | val_unsup_loss: 1.29917 |  0:00:06s\n",
      "epoch 50 | loss: 1.12796 | train_unsup_loss: 1.15372 | val_unsup_loss: 1.28351 |  0:00:06s\n",
      "epoch 51 | loss: 1.11674 | train_unsup_loss: 1.12895 | val_unsup_loss: 1.26111 |  0:00:06s\n",
      "epoch 52 | loss: 1.14419 | train_unsup_loss: 1.10861 | val_unsup_loss: 1.23964 |  0:00:06s\n",
      "epoch 53 | loss: 1.25312 | train_unsup_loss: 1.09079 | val_unsup_loss: 1.22284 |  0:00:06s\n",
      "epoch 54 | loss: 1.07393 | train_unsup_loss: 1.07546 | val_unsup_loss: 1.20675 |  0:00:06s\n",
      "epoch 55 | loss: 1.14882 | train_unsup_loss: 1.05695 | val_unsup_loss: 1.19956 |  0:00:06s\n",
      "epoch 56 | loss: 1.1411  | train_unsup_loss: 1.05058 | val_unsup_loss: 1.2065  |  0:00:07s\n",
      "epoch 57 | loss: 1.09221 | train_unsup_loss: 1.04788 | val_unsup_loss: 1.19978 |  0:00:07s\n",
      "epoch 58 | loss: 1.10163 | train_unsup_loss: 1.03863 | val_unsup_loss: 1.19231 |  0:00:07s\n",
      "epoch 59 | loss: 1.04364 | train_unsup_loss: 1.03429 | val_unsup_loss: 1.17973 |  0:00:07s\n",
      "epoch 60 | loss: 1.12698 | train_unsup_loss: 1.02535 | val_unsup_loss: 1.15063 |  0:00:07s\n",
      "epoch 61 | loss: 1.06295 | train_unsup_loss: 1.01684 | val_unsup_loss: 1.12641 |  0:00:07s\n",
      "epoch 62 | loss: 1.13213 | train_unsup_loss: 1.01518 | val_unsup_loss: 1.11409 |  0:00:07s\n",
      "epoch 63 | loss: 1.07697 | train_unsup_loss: 1.02163 | val_unsup_loss: 1.11231 |  0:00:07s\n",
      "epoch 64 | loss: 1.01806 | train_unsup_loss: 1.0185  | val_unsup_loss: 1.11118 |  0:00:08s\n",
      "epoch 65 | loss: 1.02193 | train_unsup_loss: 1.01913 | val_unsup_loss: 1.11071 |  0:00:08s\n",
      "epoch 66 | loss: 1.0443  | train_unsup_loss: 1.00399 | val_unsup_loss: 1.09094 |  0:00:08s\n",
      "epoch 67 | loss: 1.01915 | train_unsup_loss: 0.99302 | val_unsup_loss: 1.07905 |  0:00:08s\n",
      "epoch 68 | loss: 0.99804 | train_unsup_loss: 0.98591 | val_unsup_loss: 1.07189 |  0:00:08s\n",
      "epoch 69 | loss: 0.97628 | train_unsup_loss: 0.97922 | val_unsup_loss: 1.06574 |  0:00:08s\n",
      "epoch 70 | loss: 0.94768 | train_unsup_loss: 0.97285 | val_unsup_loss: 1.06191 |  0:00:08s\n",
      "epoch 71 | loss: 0.9439  | train_unsup_loss: 0.96359 | val_unsup_loss: 1.05242 |  0:00:08s\n",
      "epoch 72 | loss: 0.96309 | train_unsup_loss: 0.9448  | val_unsup_loss: 1.03714 |  0:00:09s\n",
      "epoch 73 | loss: 0.97856 | train_unsup_loss: 0.93692 | val_unsup_loss: 1.03416 |  0:00:09s\n",
      "epoch 74 | loss: 0.91515 | train_unsup_loss: 0.93032 | val_unsup_loss: 1.02817 |  0:00:09s\n",
      "epoch 75 | loss: 0.97424 | train_unsup_loss: 0.92553 | val_unsup_loss: 1.03477 |  0:00:09s\n",
      "epoch 76 | loss: 0.87122 | train_unsup_loss: 0.91494 | val_unsup_loss: 1.0215  |  0:00:09s\n",
      "epoch 77 | loss: 0.9191  | train_unsup_loss: 0.90046 | val_unsup_loss: 1.01102 |  0:00:09s\n",
      "epoch 78 | loss: 0.94172 | train_unsup_loss: 0.8987  | val_unsup_loss: 1.00526 |  0:00:09s\n",
      "epoch 79 | loss: 0.93074 | train_unsup_loss: 0.89767 | val_unsup_loss: 1.00098 |  0:00:09s\n",
      "epoch 80 | loss: 0.90238 | train_unsup_loss: 0.89005 | val_unsup_loss: 0.98657 |  0:00:10s\n",
      "epoch 81 | loss: 0.83231 | train_unsup_loss: 0.879   | val_unsup_loss: 0.96674 |  0:00:10s\n",
      "epoch 82 | loss: 0.91052 | train_unsup_loss: 0.87282 | val_unsup_loss: 0.96149 |  0:00:10s\n",
      "epoch 83 | loss: 0.82323 | train_unsup_loss: 0.87179 | val_unsup_loss: 0.95767 |  0:00:10s\n",
      "epoch 84 | loss: 0.83711 | train_unsup_loss: 0.86837 | val_unsup_loss: 0.96209 |  0:00:10s\n",
      "epoch 85 | loss: 0.90965 | train_unsup_loss: 0.87202 | val_unsup_loss: 0.96814 |  0:00:10s\n",
      "epoch 86 | loss: 0.81038 | train_unsup_loss: 0.87077 | val_unsup_loss: 0.97811 |  0:00:10s\n",
      "epoch 87 | loss: 0.8215  | train_unsup_loss: 0.86825 | val_unsup_loss: 0.97507 |  0:00:10s\n",
      "epoch 88 | loss: 0.86082 | train_unsup_loss: 0.86618 | val_unsup_loss: 0.97483 |  0:00:11s\n",
      "epoch 89 | loss: 0.8497  | train_unsup_loss: 0.8474  | val_unsup_loss: 0.95332 |  0:00:11s\n",
      "epoch 90 | loss: 0.81239 | train_unsup_loss: 0.82923 | val_unsup_loss: 0.9325  |  0:00:11s\n",
      "epoch 91 | loss: 0.83678 | train_unsup_loss: 0.81797 | val_unsup_loss: 0.92181 |  0:00:11s\n",
      "epoch 92 | loss: 0.80511 | train_unsup_loss: 0.80599 | val_unsup_loss: 0.91131 |  0:00:11s\n",
      "epoch 93 | loss: 0.74646 | train_unsup_loss: 0.79978 | val_unsup_loss: 0.90557 |  0:00:11s\n",
      "epoch 94 | loss: 0.84105 | train_unsup_loss: 0.79526 | val_unsup_loss: 0.89711 |  0:00:11s\n",
      "epoch 95 | loss: 0.85386 | train_unsup_loss: 0.80097 | val_unsup_loss: 0.89395 |  0:00:11s\n",
      "epoch 96 | loss: 0.77688 | train_unsup_loss: 0.80124 | val_unsup_loss: 0.88787 |  0:00:12s\n",
      "epoch 97 | loss: 0.78053 | train_unsup_loss: 0.79417 | val_unsup_loss: 0.8784  |  0:00:12s\n",
      "epoch 98 | loss: 0.79978 | train_unsup_loss: 0.79128 | val_unsup_loss: 0.87913 |  0:00:12s\n",
      "epoch 99 | loss: 0.76286 | train_unsup_loss: 0.79054 | val_unsup_loss: 0.88304 |  0:00:12s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_unsup_loss = 0.8784\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 0.83037 | train_auc: 0.63141 | val_auc: 0.61392 |  0:00:00s\n",
      "epoch 1  | loss: 0.67855 | train_auc: 0.65827 | val_auc: 0.65612 |  0:00:00s\n",
      "epoch 2  | loss: 0.62727 | train_auc: 0.67589 | val_auc: 0.68636 |  0:00:00s\n",
      "epoch 3  | loss: 0.62709 | train_auc: 0.69876 | val_auc: 0.70728 |  0:00:00s\n",
      "epoch 4  | loss: 0.60219 | train_auc: 0.72775 | val_auc: 0.74684 |  0:00:00s\n",
      "epoch 5  | loss: 0.57419 | train_auc: 0.75592 | val_auc: 0.78411 |  0:00:00s\n",
      "epoch 6  | loss: 0.54975 | train_auc: 0.7676  | val_auc: 0.77286 |  0:00:00s\n",
      "epoch 7  | loss: 0.52864 | train_auc: 0.70893 | val_auc: 0.67229 |  0:00:00s\n",
      "epoch 8  | loss: 0.5288  | train_auc: 0.63904 | val_auc: 0.5872  |  0:00:00s\n",
      "epoch 9  | loss: 0.51555 | train_auc: 0.59106 | val_auc: 0.51459 |  0:00:00s\n",
      "epoch 10 | loss: 0.51814 | train_auc: 0.58532 | val_auc: 0.51336 |  0:00:01s\n",
      "epoch 11 | loss: 0.49944 | train_auc: 0.58902 | val_auc: 0.50756 |  0:00:01s\n",
      "epoch 12 | loss: 0.48703 | train_auc: 0.59877 | val_auc: 0.51301 |  0:00:01s\n",
      "epoch 13 | loss: 0.48505 | train_auc: 0.61628 | val_auc: 0.52514 |  0:00:01s\n",
      "epoch 14 | loss: 0.46646 | train_auc: 0.62957 | val_auc: 0.53912 |  0:00:01s\n",
      "epoch 15 | loss: 0.47188 | train_auc: 0.64649 | val_auc: 0.55652 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_auc = 0.78411\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 5.84777 | train_unsup_loss: 343.30463| val_unsup_loss: 304.29224|  0:00:00s\n",
      "epoch 1  | loss: 4.25648 | train_unsup_loss: 285.55872| val_unsup_loss: 250.23497|  0:00:00s\n",
      "epoch 2  | loss: 3.73617 | train_unsup_loss: 103.06011| val_unsup_loss: 116.87024|  0:00:00s\n",
      "epoch 3  | loss: 3.29463 | train_unsup_loss: 48.15967| val_unsup_loss: 53.94044|  0:00:00s\n",
      "epoch 4  | loss: 2.87833 | train_unsup_loss: 31.60884| val_unsup_loss: 40.76917|  0:00:00s\n",
      "epoch 5  | loss: 2.83791 | train_unsup_loss: 27.22617| val_unsup_loss: 31.48142|  0:00:00s\n",
      "epoch 6  | loss: 2.56748 | train_unsup_loss: 17.66884| val_unsup_loss: 15.32129|  0:00:00s\n",
      "epoch 7  | loss: 2.2983  | train_unsup_loss: 8.66757 | val_unsup_loss: 6.95821 |  0:00:01s\n",
      "epoch 8  | loss: 2.16783 | train_unsup_loss: 5.88901 | val_unsup_loss: 5.87402 |  0:00:01s\n",
      "epoch 9  | loss: 2.14402 | train_unsup_loss: 5.58952 | val_unsup_loss: 5.43876 |  0:00:01s\n",
      "epoch 10 | loss: 1.91045 | train_unsup_loss: 5.07912 | val_unsup_loss: 4.95591 |  0:00:01s\n",
      "epoch 11 | loss: 2.0496  | train_unsup_loss: 5.06325 | val_unsup_loss: 5.88085 |  0:00:01s\n",
      "epoch 12 | loss: 1.89257 | train_unsup_loss: 4.77624 | val_unsup_loss: 5.90661 |  0:00:01s\n",
      "epoch 13 | loss: 1.68142 | train_unsup_loss: 4.45533 | val_unsup_loss: 5.3593  |  0:00:01s\n",
      "epoch 14 | loss: 1.74483 | train_unsup_loss: 4.53534 | val_unsup_loss: 5.22983 |  0:00:01s\n",
      "epoch 15 | loss: 1.68032 | train_unsup_loss: 4.3834  | val_unsup_loss: 5.1666  |  0:00:01s\n",
      "epoch 16 | loss: 1.57835 | train_unsup_loss: 3.87213 | val_unsup_loss: 4.55795 |  0:00:02s\n",
      "epoch 17 | loss: 1.53466 | train_unsup_loss: 3.42706 | val_unsup_loss: 3.45437 |  0:00:02s\n",
      "epoch 18 | loss: 1.63253 | train_unsup_loss: 3.19345 | val_unsup_loss: 3.26504 |  0:00:02s\n",
      "epoch 19 | loss: 1.60302 | train_unsup_loss: 3.14791 | val_unsup_loss: 3.00895 |  0:00:02s\n",
      "epoch 20 | loss: 1.64599 | train_unsup_loss: 2.82386 | val_unsup_loss: 2.75097 |  0:00:02s\n",
      "epoch 21 | loss: 1.5182  | train_unsup_loss: 2.58146 | val_unsup_loss: 2.50368 |  0:00:02s\n",
      "epoch 22 | loss: 1.57293 | train_unsup_loss: 2.42483 | val_unsup_loss: 2.43022 |  0:00:02s\n",
      "epoch 23 | loss: 1.52107 | train_unsup_loss: 2.35787 | val_unsup_loss: 2.29971 |  0:00:02s\n",
      "epoch 24 | loss: 1.48534 | train_unsup_loss: 2.2028  | val_unsup_loss: 2.19504 |  0:00:03s\n",
      "epoch 25 | loss: 1.46211 | train_unsup_loss: 2.06727 | val_unsup_loss: 2.06467 |  0:00:03s\n",
      "epoch 26 | loss: 1.4331  | train_unsup_loss: 1.90239 | val_unsup_loss: 1.91924 |  0:00:03s\n",
      "epoch 27 | loss: 1.41856 | train_unsup_loss: 1.82065 | val_unsup_loss: 1.82219 |  0:00:03s\n",
      "epoch 28 | loss: 1.44311 | train_unsup_loss: 1.76979 | val_unsup_loss: 1.78053 |  0:00:03s\n",
      "epoch 29 | loss: 1.45309 | train_unsup_loss: 1.74985 | val_unsup_loss: 1.77094 |  0:00:03s\n",
      "epoch 30 | loss: 1.37907 | train_unsup_loss: 1.72946 | val_unsup_loss: 1.73668 |  0:00:03s\n",
      "epoch 31 | loss: 1.38092 | train_unsup_loss: 1.69053 | val_unsup_loss: 1.72566 |  0:00:03s\n",
      "epoch 32 | loss: 1.4184  | train_unsup_loss: 1.66823 | val_unsup_loss: 1.69865 |  0:00:04s\n",
      "epoch 33 | loss: 1.37717 | train_unsup_loss: 1.67647 | val_unsup_loss: 1.68172 |  0:00:04s\n",
      "epoch 34 | loss: 1.49329 | train_unsup_loss: 1.66251 | val_unsup_loss: 1.66362 |  0:00:04s\n",
      "epoch 35 | loss: 1.40014 | train_unsup_loss: 1.66573 | val_unsup_loss: 1.65223 |  0:00:04s\n",
      "epoch 36 | loss: 1.34678 | train_unsup_loss: 1.64673 | val_unsup_loss: 1.65458 |  0:00:04s\n",
      "epoch 37 | loss: 1.29908 | train_unsup_loss: 1.61544 | val_unsup_loss: 1.62291 |  0:00:04s\n",
      "epoch 38 | loss: 1.39718 | train_unsup_loss: 1.57955 | val_unsup_loss: 1.59125 |  0:00:04s\n",
      "epoch 39 | loss: 1.36156 | train_unsup_loss: 1.53103 | val_unsup_loss: 1.55332 |  0:00:04s\n",
      "epoch 40 | loss: 1.39859 | train_unsup_loss: 1.50405 | val_unsup_loss: 1.54993 |  0:00:05s\n",
      "epoch 41 | loss: 1.30573 | train_unsup_loss: 1.46971 | val_unsup_loss: 1.52916 |  0:00:05s\n",
      "epoch 42 | loss: 1.26024 | train_unsup_loss: 1.44881 | val_unsup_loss: 1.51769 |  0:00:05s\n",
      "epoch 43 | loss: 1.37117 | train_unsup_loss: 1.43945 | val_unsup_loss: 1.50687 |  0:00:05s\n",
      "epoch 44 | loss: 1.27983 | train_unsup_loss: 1.44051 | val_unsup_loss: 1.50132 |  0:00:05s\n",
      "epoch 45 | loss: 1.33703 | train_unsup_loss: 1.44043 | val_unsup_loss: 1.50122 |  0:00:05s\n",
      "epoch 46 | loss: 1.26558 | train_unsup_loss: 1.44512 | val_unsup_loss: 1.50506 |  0:00:05s\n",
      "epoch 47 | loss: 1.36174 | train_unsup_loss: 1.44396 | val_unsup_loss: 1.49735 |  0:00:05s\n",
      "epoch 48 | loss: 1.19361 | train_unsup_loss: 1.44184 | val_unsup_loss: 1.49495 |  0:00:06s\n",
      "epoch 49 | loss: 1.15599 | train_unsup_loss: 1.44121 | val_unsup_loss: 1.48688 |  0:00:06s\n",
      "epoch 50 | loss: 1.28958 | train_unsup_loss: 1.43538 | val_unsup_loss: 1.51106 |  0:00:06s\n",
      "epoch 51 | loss: 1.15926 | train_unsup_loss: 1.41381 | val_unsup_loss: 1.46508 |  0:00:06s\n",
      "epoch 52 | loss: 1.23664 | train_unsup_loss: 1.39444 | val_unsup_loss: 1.44582 |  0:00:06s\n",
      "epoch 53 | loss: 1.18069 | train_unsup_loss: 1.36779 | val_unsup_loss: 1.41739 |  0:00:06s\n",
      "epoch 54 | loss: 1.17815 | train_unsup_loss: 1.35061 | val_unsup_loss: 1.40864 |  0:00:06s\n",
      "epoch 55 | loss: 1.27288 | train_unsup_loss: 1.33167 | val_unsup_loss: 1.38926 |  0:00:06s\n",
      "epoch 56 | loss: 1.25989 | train_unsup_loss: 1.32112 | val_unsup_loss: 1.37278 |  0:00:07s\n",
      "epoch 57 | loss: 1.25591 | train_unsup_loss: 1.30596 | val_unsup_loss: 1.34567 |  0:00:07s\n",
      "epoch 58 | loss: 1.13797 | train_unsup_loss: 1.29507 | val_unsup_loss: 1.34245 |  0:00:07s\n",
      "epoch 59 | loss: 1.10207 | train_unsup_loss: 1.28846 | val_unsup_loss: 1.33781 |  0:00:07s\n",
      "epoch 60 | loss: 1.00872 | train_unsup_loss: 1.2851  | val_unsup_loss: 1.33263 |  0:00:07s\n",
      "epoch 61 | loss: 1.13838 | train_unsup_loss: 1.29529 | val_unsup_loss: 1.34896 |  0:00:07s\n",
      "epoch 62 | loss: 1.12377 | train_unsup_loss: 1.30337 | val_unsup_loss: 1.35751 |  0:00:07s\n",
      "epoch 63 | loss: 1.11478 | train_unsup_loss: 1.30111 | val_unsup_loss: 1.36044 |  0:00:08s\n",
      "epoch 64 | loss: 1.03482 | train_unsup_loss: 1.28206 | val_unsup_loss: 1.33532 |  0:00:08s\n",
      "epoch 65 | loss: 1.04352 | train_unsup_loss: 1.25537 | val_unsup_loss: 1.3047  |  0:00:08s\n",
      "epoch 66 | loss: 0.96605 | train_unsup_loss: 1.23758 | val_unsup_loss: 1.27711 |  0:00:08s\n",
      "epoch 67 | loss: 0.94324 | train_unsup_loss: 1.22584 | val_unsup_loss: 1.257   |  0:00:08s\n",
      "epoch 68 | loss: 0.98933 | train_unsup_loss: 1.22193 | val_unsup_loss: 1.2478  |  0:00:08s\n",
      "epoch 69 | loss: 0.91339 | train_unsup_loss: 1.21595 | val_unsup_loss: 1.23883 |  0:00:08s\n",
      "epoch 70 | loss: 0.93917 | train_unsup_loss: 1.20536 | val_unsup_loss: 1.23343 |  0:00:08s\n",
      "epoch 71 | loss: 1.13469 | train_unsup_loss: 1.19977 | val_unsup_loss: 1.22881 |  0:00:09s\n",
      "epoch 72 | loss: 0.9358  | train_unsup_loss: 1.19518 | val_unsup_loss: 1.22485 |  0:00:09s\n",
      "epoch 73 | loss: 0.96672 | train_unsup_loss: 1.18875 | val_unsup_loss: 1.22588 |  0:00:09s\n",
      "epoch 74 | loss: 0.92059 | train_unsup_loss: 1.1854  | val_unsup_loss: 1.23133 |  0:00:09s\n",
      "epoch 75 | loss: 1.00752 | train_unsup_loss: 1.18211 | val_unsup_loss: 1.22516 |  0:00:09s\n",
      "epoch 76 | loss: 0.89517 | train_unsup_loss: 1.18387 | val_unsup_loss: 1.22462 |  0:00:09s\n",
      "epoch 77 | loss: 0.9794  | train_unsup_loss: 1.18678 | val_unsup_loss: 1.22321 |  0:00:09s\n",
      "epoch 78 | loss: 0.89078 | train_unsup_loss: 1.1773  | val_unsup_loss: 1.20619 |  0:00:09s\n",
      "epoch 79 | loss: 0.90967 | train_unsup_loss: 1.17432 | val_unsup_loss: 1.19974 |  0:00:10s\n",
      "epoch 80 | loss: 0.83364 | train_unsup_loss: 1.16498 | val_unsup_loss: 1.18863 |  0:00:10s\n",
      "epoch 81 | loss: 0.94141 | train_unsup_loss: 1.16894 | val_unsup_loss: 1.18472 |  0:00:10s\n",
      "epoch 82 | loss: 0.84874 | train_unsup_loss: 1.16812 | val_unsup_loss: 1.19069 |  0:00:10s\n",
      "epoch 83 | loss: 0.84588 | train_unsup_loss: 1.1707  | val_unsup_loss: 1.19761 |  0:00:10s\n",
      "epoch 84 | loss: 0.87383 | train_unsup_loss: 1.17362 | val_unsup_loss: 1.20464 |  0:00:10s\n",
      "epoch 85 | loss: 0.90643 | train_unsup_loss: 1.16912 | val_unsup_loss: 1.20159 |  0:00:10s\n",
      "epoch 86 | loss: 0.94513 | train_unsup_loss: 1.16888 | val_unsup_loss: 1.19965 |  0:00:11s\n",
      "epoch 87 | loss: 0.88596 | train_unsup_loss: 1.15545 | val_unsup_loss: 1.18781 |  0:00:11s\n",
      "epoch 88 | loss: 0.9139  | train_unsup_loss: 1.15742 | val_unsup_loss: 1.19616 |  0:00:11s\n",
      "epoch 89 | loss: 0.9174  | train_unsup_loss: 1.16592 | val_unsup_loss: 1.20555 |  0:00:11s\n",
      "epoch 90 | loss: 0.89963 | train_unsup_loss: 1.15619 | val_unsup_loss: 1.19062 |  0:00:11s\n",
      "epoch 91 | loss: 0.87227 | train_unsup_loss: 1.13745 | val_unsup_loss: 1.16632 |  0:00:11s\n",
      "epoch 92 | loss: 0.87976 | train_unsup_loss: 1.12471 | val_unsup_loss: 1.14562 |  0:00:11s\n",
      "epoch 93 | loss: 0.89173 | train_unsup_loss: 1.11345 | val_unsup_loss: 1.12623 |  0:00:11s\n",
      "epoch 94 | loss: 0.86643 | train_unsup_loss: 1.11942 | val_unsup_loss: 1.13921 |  0:00:12s\n",
      "epoch 95 | loss: 0.85717 | train_unsup_loss: 1.1192  | val_unsup_loss: 1.13925 |  0:00:12s\n",
      "epoch 96 | loss: 0.85975 | train_unsup_loss: 1.10801 | val_unsup_loss: 1.13399 |  0:00:12s\n",
      "epoch 97 | loss: 0.89127 | train_unsup_loss: 1.08915 | val_unsup_loss: 1.11624 |  0:00:12s\n",
      "epoch 98 | loss: 0.85041 | train_unsup_loss: 1.07106 | val_unsup_loss: 1.09478 |  0:00:12s\n",
      "epoch 99 | loss: 0.83228 | train_unsup_loss: 1.0555  | val_unsup_loss: 1.08693 |  0:00:12s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_unsup_loss = 1.08693\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 0.8853  | train_auc: 0.56994 | val_auc: 0.58693 |  0:00:00s\n",
      "epoch 1  | loss: 0.67401 | train_auc: 0.57151 | val_auc: 0.59662 |  0:00:00s\n",
      "epoch 2  | loss: 0.62384 | train_auc: 0.56718 | val_auc: 0.60336 |  0:00:00s\n",
      "epoch 3  | loss: 0.6264  | train_auc: 0.58377 | val_auc: 0.61512 |  0:00:00s\n",
      "epoch 4  | loss: 0.60432 | train_auc: 0.59989 | val_auc: 0.61701 |  0:00:00s\n",
      "epoch 5  | loss: 0.56807 | train_auc: 0.62512 | val_auc: 0.6338  |  0:00:00s\n",
      "epoch 6  | loss: 0.54215 | train_auc: 0.63054 | val_auc: 0.63156 |  0:00:00s\n",
      "epoch 7  | loss: 0.54071 | train_auc: 0.63309 | val_auc: 0.63658 |  0:00:00s\n",
      "epoch 8  | loss: 0.55206 | train_auc: 0.64971 | val_auc: 0.64476 |  0:00:00s\n",
      "epoch 9  | loss: 0.54285 | train_auc: 0.68331 | val_auc: 0.67574 |  0:00:01s\n",
      "epoch 10 | loss: 0.53149 | train_auc: 0.7013  | val_auc: 0.69415 |  0:00:01s\n",
      "epoch 11 | loss: 0.51235 | train_auc: 0.71892 | val_auc: 0.72854 |  0:00:01s\n",
      "epoch 12 | loss: 0.49661 | train_auc: 0.72511 | val_auc: 0.72611 |  0:00:01s\n",
      "epoch 13 | loss: 0.48914 | train_auc: 0.73184 | val_auc: 0.72962 |  0:00:01s\n",
      "epoch 14 | loss: 0.47981 | train_auc: 0.73323 | val_auc: 0.71992 |  0:00:01s\n",
      "epoch 15 | loss: 0.49638 | train_auc: 0.73392 | val_auc: 0.71992 |  0:00:01s\n",
      "epoch 16 | loss: 0.49916 | train_auc: 0.73449 | val_auc: 0.71255 |  0:00:01s\n",
      "epoch 17 | loss: 0.48147 | train_auc: 0.73166 | val_auc: 0.71219 |  0:00:01s\n",
      "epoch 18 | loss: 0.47745 | train_auc: 0.73199 | val_auc: 0.71309 |  0:00:01s\n",
      "epoch 19 | loss: 0.45264 | train_auc: 0.73862 | val_auc: 0.72028 |  0:00:02s\n",
      "epoch 20 | loss: 0.44933 | train_auc: 0.73716 | val_auc: 0.73105 |  0:00:02s\n",
      "epoch 21 | loss: 0.44614 | train_auc: 0.74432 | val_auc: 0.73707 |  0:00:02s\n",
      "epoch 22 | loss: 0.44682 | train_auc: 0.75374 | val_auc: 0.7412  |  0:00:02s\n",
      "epoch 23 | loss: 0.42648 | train_auc: 0.76606 | val_auc: 0.75    |  0:00:02s\n",
      "epoch 24 | loss: 0.43043 | train_auc: 0.77371 | val_auc: 0.75377 |  0:00:02s\n",
      "epoch 25 | loss: 0.42375 | train_auc: 0.78235 | val_auc: 0.74865 |  0:00:02s\n",
      "epoch 26 | loss: 0.42196 | train_auc: 0.79075 | val_auc: 0.75278 |  0:00:02s\n",
      "epoch 27 | loss: 0.4153  | train_auc: 0.7946  | val_auc: 0.75835 |  0:00:02s\n",
      "epoch 28 | loss: 0.42159 | train_auc: 0.79884 | val_auc: 0.76302 |  0:00:02s\n",
      "epoch 29 | loss: 0.41116 | train_auc: 0.79932 | val_auc: 0.7641  |  0:00:03s\n",
      "epoch 30 | loss: 0.41096 | train_auc: 0.80524 | val_auc: 0.76859 |  0:00:03s\n",
      "epoch 31 | loss: 0.40752 | train_auc: 0.81224 | val_auc: 0.77739 |  0:00:03s\n",
      "epoch 32 | loss: 0.41084 | train_auc: 0.81881 | val_auc: 0.77918 |  0:00:03s\n",
      "epoch 33 | loss: 0.40843 | train_auc: 0.82718 | val_auc: 0.79337 |  0:00:03s\n",
      "epoch 34 | loss: 0.40227 | train_auc: 0.83103 | val_auc: 0.79535 |  0:00:03s\n",
      "epoch 35 | loss: 0.40095 | train_auc: 0.83103 | val_auc: 0.78861 |  0:00:03s\n",
      "epoch 36 | loss: 0.40015 | train_auc: 0.82476 | val_auc: 0.77478 |  0:00:03s\n",
      "epoch 37 | loss: 0.38513 | train_auc: 0.81677 | val_auc: 0.77371 |  0:00:03s\n",
      "epoch 38 | loss: 0.38297 | train_auc: 0.82077 | val_auc: 0.77227 |  0:00:03s\n",
      "epoch 39 | loss: 0.3852  | train_auc: 0.82125 | val_auc: 0.76149 |  0:00:04s\n",
      "epoch 40 | loss: 0.37854 | train_auc: 0.82053 | val_auc: 0.76185 |  0:00:04s\n",
      "epoch 41 | loss: 0.38476 | train_auc: 0.82015 | val_auc: 0.76365 |  0:00:04s\n",
      "epoch 42 | loss: 0.38637 | train_auc: 0.8206  | val_auc: 0.78053 |  0:00:04s\n",
      "epoch 43 | loss: 0.38674 | train_auc: 0.82209 | val_auc: 0.77514 |  0:00:04s\n",
      "epoch 44 | loss: 0.37934 | train_auc: 0.81798 | val_auc: 0.78197 |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_auc = 0.79535\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "RANDOM_STATE = hash(\"fuck yeah!\") % (2 ** 32 - 1)\n",
    "print(RANDOM_STATE)\n",
    "\n",
    "\n",
    "train_path = osp.join('../data/processed/', \"train.csv\")\n",
    "train_df = pd.read_csv(train_path, index_col=\"PassengerId\")\n",
    "y = train_df[\"survived\"].values\n",
    "X = train_df.drop([\"survived\"], axis=1).values\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "val_mean_aucs = []\n",
    "train_mean_aucs = []\n",
    "best_epochs = []\n",
    "best_model = None\n",
    "best_val_auc = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[test_index]\n",
    "    y_train, y_val = y[train_index], y[test_index]\n",
    "    pretrainer = TabNetPretrainer(seed=RANDOM_STATE)\n",
    "    pretrainer.fit(\n",
    "        X_train, \n",
    "        eval_set=[X_train, X_val], \n",
    "        eval_name=['train', 'val']\n",
    "    )\n",
    "    model = TabNetClassifier(\n",
    "        seed=RANDOM_STATE,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "        eval_name=['train', 'val'],\n",
    "        from_unsupervised=pretrainer\n",
    "    )\n",
    "    valpreds = model.predict_proba(X_val)[:, 1]\n",
    "    trainpreds = model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    val_auc = roc_auc_score(y_val, valpreds)\n",
    "    train_auc = roc_auc_score(y_train, trainpreds)\n",
    "    \n",
    "    if val_auc > best_val_auc:\n",
    "        best_model = model\n",
    "        best_val_auc = val_auc\n",
    "    \n",
    "    val_mean_aucs.append(val_auc)\n",
    "    train_mean_aucs.append(train_auc)\n",
    "    best_epochs.append(model.best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 34 |Â train AUC 0.8288465007215007 | val AUC 0.7727840199750312\n",
      "Best epoch: 16 |Â train AUC 0.7177703081232494 | val AUC 0.802014652014652\n",
      "Best epoch: 19 |Â train AUC 0.8160555270381861 | val AUC 0.8274436090225564\n",
      "Best epoch: 5 |Â train AUC 0.7559170588636519 | val AUC 0.7841068917018283\n",
      "Best epoch: 34 |Â train AUC 0.8310261777747766 | val AUC 0.7953484195402298\n"
     ]
    }
   ],
   "source": [
    "train_mean_aucs = np.array(train_mean_aucs)\n",
    "val_mean_aucs = np.array(val_mean_aucs)\n",
    "best_epochs = np.array(best_epochs)\n",
    "\n",
    "for be, t, v in zip(best_epochs, train_mean_aucs, val_mean_aucs):\n",
    "    print(f\"Best epoch: {be} |Â train AUC {t} | val AUC {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7899231145042729\n",
      "0.7963395184508596\n"
     ]
    }
   ],
   "source": [
    "print(train_mean_aucs.mean())\n",
    "print(val_mean_aucs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.8160565813627036\n",
      "0.4924383\n"
     ]
    }
   ],
   "source": [
    "preds = best_model.predict_proba(X)[:, 1]\n",
    "auc = roc_auc_score(y, preds)\n",
    "print(f\"AUC = {auc}\")\n",
    "fpr, tpr, thr = roc_curve(y, preds)\n",
    "optimial_thr = thr[np.argmax(tpr - fpr)]\n",
    "print(optimial_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = (preds > optimial_thr).astype(int)\n",
    "(y_hat == y).sum() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at best_model_train_auc=81_acc=77.zip\n",
      "Device used : cpu\n",
      "Device used : cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TabNetClassifier(n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1, n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=2785828985, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=9, output_dim=2, device_name='auto')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'best_model_train_auc=81_acc=77'\n",
    "best_model.save_model(name)\n",
    "loaded_model = TabNetClassifier()\n",
    "loaded_model.load_model(f\"{name}.zip\")\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(osp.join('../data/processed/', \"test.csv\"), index_col=\"PassengerId\")\n",
    "X_test = test_df.values\n",
    "probs = loaded_model.predict_proba(X_test)[:, 1]\n",
    "y_hat = (probs > optimial_thr).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data={\n",
    "        'PassengerId': test_df.index,\n",
    "        'Survived': y_hat,\n",
    "    },\n",
    ").to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "698487d03f837c12ed9acd7a246542becdf3fe66990446d81ee4bf24aeef78ab"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('titanic': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
